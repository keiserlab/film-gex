{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiLMNetwork(pl.LightningModule):\n",
    "    def __init__(self, inputs_sz, conds_sz):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.inputs_emb = self.generate_layers(in_sz=inputs_sz, layers=[512], out_sz=32, ps=None, use_bn=True, bn_final=True)\n",
    "        self.conds_emb = self.generate_layers(in_sz=conds_sz, layers=[], out_sz=32, ps=None, use_bn=True, bn_final=False)\n",
    "        self.film_1 = self.film_generator(in_sz=32, layers=[], out_sz=32, ps=None, use_bn=False, bn_final=False)\n",
    "        self.block_1 = self.generate_layers(in_sz=32, layers=[16], out_sz=16, ps=None, use_bn=True, bn_final=True)\n",
    "        self.film_2 = self.film_generator(in_sz=16, layers=[], out_sz=16, ps=None, use_bn=False, bn_final=False)\n",
    "        self.block_2 = self.generate_layers(in_sz=16, layers=[8], out_sz=1, ps=None, use_bn=True, bn_final=False)\n",
    "    \n",
    "    def film_generator(self, *args, **kwargs):\n",
    "        gamma = self.generate_layers(*args, **kwargs)\n",
    "        beta = self.generate_layers(*args, **kwargs)\n",
    "        return gamma, beta\n",
    "    \n",
    "    def generate_layers(self, in_sz, layers, out_sz, ps, use_bn, bn_final):\n",
    "        if ps is None: ps = [0]*len(layers) \n",
    "        else: ps = ps*len(layers)\n",
    "        sizes = self.get_sizes(in_sz, layers, out_sz)\n",
    "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n",
    "        layers = []\n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += self.bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        block = nn.Sequential(*layers)\n",
    "        return block\n",
    "    \n",
    "    def get_sizes(self, in_sz, layers, out_sz):\n",
    "        return [in_sz] + layers + [out_sz]\n",
    "    \n",
    "    def bn_drop_lin(self, n_in:int, n_out:int, bn:bool=True, p:float=0., actn:nn.Module=None):\n",
    "        \"`n_in`->bn->dropout->linear(`n_in`,`n_out`)->`actn`\"\n",
    "        layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "        if p != 0: layers.append(nn.Dropout(p))\n",
    "        layers.append(nn.Linear(n_in, n_out))\n",
    "        if actn is not None: layers.append(actn)\n",
    "        return layers\n",
    "    \n",
    "    def forward(self, inputs, conds):\n",
    "        return self.conds_emb(conds)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, conds, y = batch\n",
    "        input_emb = self.input_emb(inputs)\n",
    "        conds_emb = self.conds_emb(conds)\n",
    "        gamma_1, beta_1 = self.film_1(conds_emb)\n",
    "        x = input_emb * gamma_1 + beta_1\n",
    "        x = self.block_1(x)\n",
    "        gamma_2, beta_2 = self.film_2(conds_emb)\n",
    "        x = x * gamma_2 + beta_2\n",
    "        y_hat = self.block_2(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        result = pl.TrainResult(minimize=loss)\n",
    "        result.log('train_loss', loss)\n",
    "        return result\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, conds, y = batch\n",
    "        input_emb = self.input_emb(inputs)\n",
    "        conds_emb = self.conds_emb(conds)\n",
    "        gamma_1, beta_1 = self.film_1(conds_emb)\n",
    "        x = input_emb * gamma_1 + beta_1\n",
    "        x = self.block_1(x)\n",
    "        gamma_2, beta_2 = self.film_2(conds_emb)\n",
    "        x = x * gamma_2 + beta_2\n",
    "        y_hat = self.block_2(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log('val_loss', loss)\n",
    "        result.log('val_acc', accuracy(y_hat, y))\n",
    "        return result\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, conds, y = batch\n",
    "        input_emb = self.input_emb(inputs)\n",
    "        conds_emb = self.conds_emb(conds)\n",
    "        gamma_1, beta_1 = self.film_1(conds_emb)\n",
    "        x = input_emb * gamma_1 + beta_1\n",
    "        x = self.block_1(x)\n",
    "        gamma_2, beta_2 = self.film_2(conds_emb)\n",
    "        x = x * gamma_2 + beta_2\n",
    "        y_hat = self.block_2(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log('test_loss', loss)\n",
    "        result.log('test_acc', accuracy(y_hat, y))\n",
    "        return result\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = FiLMNetwork(978, 513)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FiLMNetwork(\n",
       "  (inputs_emb): Sequential(\n",
       "    (0): Linear(in_features=978, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=512, out_features=32, bias=True)\n",
       "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conds_emb): Sequential(\n",
       "    (0): Linear(in_features=513, out_features=32, bias=True)\n",
       "  )\n",
       "  (block_1): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (block_2): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=8, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lightning]",
   "language": "python",
   "name": "conda-env-lightning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
