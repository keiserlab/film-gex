{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join(os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.sklearns import R2Score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Custom\n",
    "from project.film_model import LinearBlock, FiLMGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FiLM Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiLMNetwork(pl.LightningModule):\n",
    "    def __init__(self, inputs_sz, conds_sz, learning_rate=1e-2, metric=R2Score()):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.metric = metric\n",
    "        self.inputs_emb = LinearBlock(in_sz=inputs_sz, layers=[512,256,128,64], out_sz=32, ps=None, use_bn=True, bn_final=True)\n",
    "        self.conds_emb = LinearBlock(in_sz=conds_sz, layers=[], out_sz=32, ps=None, use_bn=True, bn_final=False)\n",
    "        self.film_1 = FiLMGenerator(in_sz=self.conds_emb.out_sz, layers=[], out_sz=32, ps=None, use_bn=False, bn_final=False)\n",
    "        self.block_1 = LinearBlock(in_sz=self.film_1.out_sz, layers=[16], out_sz=16, ps=None, use_bn=True, bn_final=True)\n",
    "        self.film_2 = FiLMGenerator(in_sz=self.conds_emb.out_sz, layers=[], out_sz=16, ps=None, use_bn=False, bn_final=False)\n",
    "        self.block_2 = LinearBlock(in_sz=self.film_2.out_sz, layers=[8], out_sz=1, ps=None, use_bn=True, bn_final=False)\n",
    "    \n",
    "    def _forward(self, batch, batch_idx):\n",
    "        inputs, conds, y = batch\n",
    "        input_emb = self.inputs_emb(inputs)\n",
    "        conds_emb = self.conds_emb(conds)\n",
    "        gamma_1, beta_1 = self.film_1(conds_emb)\n",
    "        x = input_emb * gamma_1 + beta_1\n",
    "        x = self.block_1(x)\n",
    "        gamma_2, beta_2 = self.film_2(conds_emb)\n",
    "        x = x * gamma_2 + beta_2\n",
    "        y_hat = self.block_2(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, conds):\n",
    "        return self.conds_emb(conds)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._forward(batch, batch_idx)\n",
    "        result = pl.TrainResult(minimize=loss)\n",
    "        result.log('train_loss', loss)\n",
    "        return result\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._forward(batch, batch_idx)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log('val_loss', loss, on_step=True)\n",
    "        result.log('val_r2', self.metric(y_hat, y), on_step=True)\n",
    "        return result\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._forward(batch, batch_idx)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log('test_loss', loss, on_step=True)\n",
    "        result.log('test_r2', self.metric(y_hat, y), on_step=True)\n",
    "        return result\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.film_model import FiLMNetwork, ConcatNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'learning_rate' and 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6f2922041c75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFiLMNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m978\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m513\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'learning_rate' and 'batch_size'"
     ]
    }
   ],
   "source": [
    "bar = FiLMNetwork(978, 513)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"conds_sz\":      513\n",
       "\"inputs_sz\":     978\n",
       "\"learning_rate\": 0.01\n",
       "\"metric\":        R2Score()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512 / 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatNetwork(pl.LightningModule):\n",
    "    def __init__(self, inputs_sz, conds_sz, learning_rate=1e-2, metric=R2Score()):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.metric = metric\n",
    "        self.inputs_emb = LinearBlock(in_sz=inputs_sz, layers=[512,256,128,64], out_sz=32, ps=None, use_bn=True, bn_final=True)\n",
    "        self.conds_emb = LinearBlock(in_sz=conds_sz, layers=[], out_sz=32, ps=None, use_bn=True, bn_final=False)\n",
    "        self.block_1 = LinearBlock(in_sz=self.inputs_emb.out_sz + self.conds_emb.out_sz, layers=[16], out_sz=16, ps=None, use_bn=True, bn_final=True)\n",
    "        self.block_2 = LinearBlock(in_sz=self.block_1.out_sz, layers=[8], out_sz=1, ps=None, use_bn=True, bn_final=False)\n",
    "    \n",
    "    def _forward(self, batch, batch_idx):\n",
    "        inputs, conds, y = batch\n",
    "        input_emb = self.inputs_emb(inputs)\n",
    "        conds_emb = self.conds_emb(conds)\n",
    "        x = torch.cat([input_emb, conds_emb], dim=1)\n",
    "        x = self.block_1(x)\n",
    "        y_hat = self.block_2(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, conds):\n",
    "        return self.conds_emb(conds)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._forward(batch, batch_idx)\n",
    "        result = pl.TrainResult(minimize=loss)\n",
    "        result.log('train_loss', loss)\n",
    "        return result\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._forward(batch, batch_idx)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log('val_loss', loss, on_step=True)\n",
    "        result.log('val_r2', self.metric(y_hat, y), on_step=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._forward(batch, batch_idx)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log('test_loss', loss, on_step=True)\n",
    "        result.log('test_r2', self.metric(y_hat, y), on_step=True)\n",
    "        return result\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lightning]",
   "language": "python",
   "name": "conda-env-lightning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
